{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import openai\n",
    "from openai import OpenAI \n",
    "from scipy import spatial  # вычисляет сходство векторов\n",
    "import tiktoken  # для подсчета токенов\n",
    "\n",
    "import API_KEYS\n",
    "\n",
    "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # Модель токенизации от OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_path = \"embeddings.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)\n",
    "\n",
    "# Конвертируем наши эмбединги из строк в списки\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция поиска\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str, # пользовательский запрос\n",
    "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), # функция схожести, косинусное расстояние\n",
    "    top_n: int = 100 # выбор лучших n-результатов\n",
    ") -> tuple[list[str], list[float]]: # Функция возвращает кортеж двух списков, первый содержит строки, второй - числа с плавающей запятой\n",
    "    \"\"\"Возвращает строки и схожести, отсортированные от большего к меньшему\"\"\"\n",
    "\n",
    "    # Отправляем в OpenAI API пользовательский запрос для токенизации\n",
    "    query_embedding_response = openai.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "\n",
    "    # Получен токенизированный пользовательский запрос\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Сравниваем пользовательский запрос с каждой токенизированной строкой DataFrame\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Сортируем по убыванию схожести полученный список\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Преобразовываем наш список в кортеж из списков\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "\n",
    "    # Возвращаем n лучших результатов\n",
    "    return strings[:top_n], relatednesses[:top_n]\n",
    "\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"Положение товара в поиске\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Возвращает число токенов в строке для заданной модели\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Функция формирования запроса к chatGPT по пользовательскому вопросу и базе знаний\n",
    "def query_message(\n",
    "    query: str, # пользовательский запрос\n",
    "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
    "    model: str, # модель\n",
    "    token_budget: int # ограничение на число отсылаемых токенов в модель\n",
    ") -> str:\n",
    "    \"\"\"Возвращает сообщение для GPT с соответствующими исходными текстами, извлеченными из фрейма данных (базы знаний).\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df) # функция ранжирования базы знаний по пользовательскому запросу\n",
    "    # Шаблон инструкции для chatGPT\n",
    "    message = 'Use the following parts of the Ozon API documentation to answer the following question. If the answer is not found in the documentation, write \"Я не смог найти ответ\"'\n",
    "    # Шаблон для вопроса\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # Добавляем к сообщению для chatGPT релевантные строки из базы знаний, пока не выйдем за допустимое число токенов\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nParts of the Ozon API:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (num_tokens(message + next_article + question, model=model) > token_budget):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str, # пользовательский запрос\n",
    "    df: pd.DataFrame = df, # DataFrame со столбцами text и embedding (база знаний)\n",
    "    model: str = GPT_MODEL, # модель\n",
    "    token_budget: int = 4096 - 500, # ограничение на число отсылаемых токенов в модель\n",
    "    print_message: bool = False, # нужно ли выводить сообщение перед отправкой\n",
    ") -> str:\n",
    "    \"\"\"Отвечает на вопрос, используя GPT и базу знаний.\"\"\"\n",
    "    # Формируем сообщение к chatGPT (функция выше)\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    # Если параметр True, то выводим сообщение\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You're answering questions about the Ozon API. Отвечай по русски.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0 # гиперпараметр степени случайности при генерации текста. Влияет на то, как модель выбирает следующее слово в последовательности.\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Telegram-consultant-on-Ozon-API-6P5ZJ6AU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
